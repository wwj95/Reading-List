# Reading list

## Adversarial Attack Algorithm:
###	White-box attack:

- [Intriguing properties of neural networks (L-BFGM)](https://arxiv.org/abs/1312.6199)
- [Explaining and Harnessing Adversarial Examples(FGSM)](https://arxiv.org/abs/1412.6572)
- [Adversarial diversity and hard positive generation](https://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w12/html/Rozsa_Adversarial_Diversity_and_CVPR_2016_paper.html)
- [Adversarial Machine Learning at Scale](https://arxiv.org/abs/1611.01236)
- [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)
- [The Limitations of Deep Learning in Adversarial Settings(JSMA)](https://arxiv.org/abs/1511.07528)
- [DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/abs/1511.04599)
- [Towards Evaluating the Robustness of Neural Networks(C&W)](https://arxiv.org/abs/1608.04644)


###	Black-box attack:
- [Practical black-box attacks against machine learning](https://arxiv.org/abs/1602.02697)
- [Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models](https://arxiv.org/abs/1712.04248)
- [ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://arxiv.org/abs/1708.03999)

### Adversarial example in real-world:
- [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)

## Adversarial Defense Algorithm:

### Network Distillation

- [Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks](https://arxiv.org/abs/1511.04508)

### Adversarial Training

- [Ensemble Adversarial Training: Attacks and Defenses](https://arxiv.org/abs/1705.07204)
- [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)

###	Adversarial detection


-	[Adversarial and Clean Data Are Not Twin](https://arxiv.org/abs/1704.04960)
-	[On Detecting Adversarial Perturbations](https://arxiv.org/abs/1702.04267)
-	[Detecting Adversarial Samples from Artifacts](https://arxiv.org/abs/1703.00410)
-	[MagNet: A Two-Pronged Defense against Adversarial Examples](https://dl.acm.org/doi/abs/10.1145/3133956.3134057)
- [PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples](https://arxiv.org/abs/1710.10766)
- [Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks](https://arxiv.org/abs/1704.01155)
-	[Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods](https://arxiv.org/abs/1705.07263) 

###	Gradient Masking

- [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)

### Current state of the art

- [Towards the first adversarially robust neural network model on MNIST](https://arxiv.org/abs/1805.09190)

### Certified robustness

- [On the Effectiveness of Interval Bound Propagation (IBP) for Training Verifiably Robust Models](https://arxiv.org/abs/1810.12715)
- [Certified robustness with differential privacy](https://arxiv.org/abs/1802.03471)
- [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)
- [Certified Adversarial Robustness with Additive Noise](https://arxiv.org/abs/1809.03113)
- [On Evaluating Adversarial Robustness](https://arxiv.org/abs/1902.06705)

