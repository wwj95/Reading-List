# Qualification Reading list(1)

## Adversarial Attack Algorithm:
###	White-box attack:

- [Evasion Attacks against Machine Learning at Test Time(First work)](https://arxiv.org/abs/1708.06131)
- [Intriguing properties of neural networks (L-BFGM)](https://arxiv.org/abs/1312.6199)
- [Explaining and Harnessing Adversarial Examples(FGSM)](https://arxiv.org/abs/1412.6572)
- [The Limitations of Deep Learning in Adversarial Settings(JSMA)](https://arxiv.org/abs/1511.07528)
- [DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/abs/1511.04599)
- [Towards Evaluating the Robustness of Neural Networks(C&W)](https://arxiv.org/abs/1608.04644)


###	Black-box attack:

- [Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models](https://arxiv.org/abs/1712.04248)
- [ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://arxiv.org/abs/1708.03999)

### Adversarial example in real-world:

- [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)
- [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)

## Adversarial Defense Algorithm:

### Network Distillation

- [Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks](https://arxiv.org/abs/1511.04508)

### Adversarial Training

- [Ensemble Adversarial Training: Attacks and Defenses](https://arxiv.org/abs/1705.07204)
- [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)

###	Adversarial detection

-	[MagNet: A Two-Pronged Defense against Adversarial Examples](https://dl.acm.org/doi/abs/10.1145/3133956.3134057)
-	[Adversarial and Clean Data Are Not Twin](https://arxiv.org/abs/1704.04960)
-	[On Detecting Adversarial Perturbations](https://arxiv.org/abs/1702.04267)
-	[Detecting Adversarial Samples from Artifacts](https://arxiv.org/abs/1703.00410)
-	[Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods](https://arxiv.org/abs/1705.07263) 

###	Gradient Masking

- [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)

### Current state of the art

- [Towards the first adversarially robust neural network model on MNIST](https://arxiv.org/abs/1805.09190)
- [On Evaluating Adversarial Robustness](https://arxiv.org/abs/1902.06705)

### Certified robustness

- [On the Effectiveness of Interval Bound Propagation (IBP) for Training Verifiably Robust Models](https://arxiv.org/abs/1810.12715)
- [Certified robustness with differential privacy](https://arxiv.org/abs/1802.03471)
- [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)
- [Certified Adversarial Robustness with Additive Noise](https://arxiv.org/abs/1809.03113)


##	Text Domain
-   [Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency](https://www.aclweb.org/anthology/P19-1103/)
-	[Interpretable Adversarial Perturbation in Input Embedding Space for Text](https://www.ijcai.org/Proceedings/2018/0601.pdf)
-	[Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack](https://arxiv.org/abs/1908.06083)
-	[Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation](https://arxiv.org/abs/1909.01492)
-	[Certified Robustness to Adversarial Word Substitutions](https://www.aclweb.org/anthology/D19-1423.pdf)
